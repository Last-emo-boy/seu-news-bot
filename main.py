import asyncio
import aiohttp
import json
from pathlib import Path
from datetime import datetime
from astrbot.api.all import *
from astrbot.api import logger
from bs4 import BeautifulSoup
from .news_db import NewsDB

# è¯·æ±‚å¤´ï¼Œé˜²æ­¢è¢«å°
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# å®šä¹‰å„ç»„ä¿¡æ¯ï¼ˆæ–°é—»æ¥æºåŠæ ç›®ï¼‰
GROUPS = [
    {
        "source": "æ•™åŠ¡å¤„",
        "base_url": "https://jwc.seu.edu.cn",
        "categories": {
            "zxdt": "zxdt"
        },
        "container_id": "wp_news_w8"  # è¡¨æ ¼ç»“æ„
    },
    {
        "source": "å¤–å›½è¯­å­¦é™¢",
        "base_url": "https://sfl.seu.edu.cn",
        "categories": {
            "å­¦é™¢å…¬å‘Š": "9827"
        },
        "container_id": "wp_news_w6"  # åˆ—è¡¨ç»“æ„
    },
    {
        "source": "ç”µå­ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢",
        "base_url": "https://electronic.seu.edu.cn",
        "categories": {
            "é€šçŸ¥å…¬å‘Š": "11484"
        },
        "container_id": "wp_news_w6"  # åˆ—è¡¨ç»“æ„
    }
]

# æŒä¹…åŒ–è‡ªåŠ¨æ›´æ–°é€šçŸ¥åˆ—è¡¨çš„ JSON æ–‡ä»¶è·¯å¾„
AUTO_NOTIFY_FILE = Path(__file__).parent / "auto_notify.json"

def load_auto_notify_origins():
    if AUTO_NOTIFY_FILE.exists():
        try:
            with open(AUTO_NOTIFY_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return set(data)
        except Exception as e:
            logger.error(f"åŠ è½½è‡ªåŠ¨é€šçŸ¥åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
    return set()

def save_auto_notify_origins(origins: set):
    try:
        with open(AUTO_NOTIFY_FILE, "w", encoding="utf-8") as f:
            json.dump(list(origins), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜è‡ªåŠ¨é€šçŸ¥åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")

def get_page_url(base_url, identifier, page):
    """
    æ„é€ é¡µé¢ URLï¼š
      - ç¬¬ä¸€é¡µä¸º {base_url}/{identifier}/list.htm
      - å…¶å®ƒé¡µä¸º {base_url}/{identifier}/list{page}.htm
    """
    if page == 1:
        return f"{base_url}/{identifier}/list.htm"
    else:
        return f"{base_url}/{identifier}/list{page}.htm"

@register("SEUåŠ©æ‰‹", "æ–°é—»è®¢é˜…ä¸æŸ¥è¯¢æ’ä»¶ï¼Œæ”¯æŒå¤šæ¥æºæŸ¥è¯¢ï¼Œè‡ªåŠ¨è¾“å‡ºæœ€æ–°æ–°é—»åŠå…¨é‡æ›´æ–°", "1.0.2", "https://github.com/Last-emo-boy/seu-news-bot")
class NewsPlugin(Star):
    def __init__(self, context: Context, config: dict):
        """
        åˆå§‹åŒ–æ—¶æ¥æ”¶é…ç½®æ–‡ä»¶ï¼ˆé€šè¿‡ _conf_schema.jsonï¼‰ï¼Œé…ç½®é¡¹åŒ…æ‹¬ï¼š
          - check_interval: æ£€æŸ¥æ›´æ–°çš„é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 3600 ç§’
          - notify_origin: ï¼ˆå¯é€‰ï¼‰è¡¥å……çš„é€šçŸ¥ç›®æ ‡ï¼Œä¼šè¯æ ‡è¯†ï¼ˆä¸å½±å“è‡ªåŠ¨è®¢é˜…ï¼‰
        """
        super().__init__(context)
        self.config = config
        self.db = NewsDB()
        self.auto_notify_origins = load_auto_notify_origins()
        interval = self.config.get("check_interval", 3600)
        logger.info(f"æ–°é—»æ’ä»¶å¯åŠ¨ï¼Œæ›´æ–°é—´éš”ä¸º {interval} ç§’")
        asyncio.create_task(self.scheduled_check(interval=interval))
    
    async def scheduled_check(self, interval: int):
        """
        å®šæ—¶ä»»åŠ¡ï¼šæ¯æ¬¡æ£€æŸ¥æ›´æ–°åï¼Œ
          è‹¥æ£€æµ‹åˆ°æ–°æ–°é—»ä¸”å­˜åœ¨è‡ªåŠ¨è®¢é˜…ä¼šè¯ï¼Œåˆ™å‘æ‰€æœ‰è®¢é˜…ä¼šè¯æ¨é€æœ€æ–°æ–°é—»ã€‚
        """
        while True:
            new_news = await self.check_updates(force_update=False)
            if new_news and self.auto_notify_origins:
                msg_text = f"æ£€æµ‹åˆ° {len(new_news)} æ¡æ–°æ–°é—»ï¼š\n\n"
                for src, cat, title, url, date_str in new_news:
                    msg_text += f"ã€{src} - {cat}ã€‘ {title}\né“¾æ¥ï¼š{url}\nå‘å¸ƒæ—¥æœŸï¼š{date_str}\n\n"
                for origin in self.auto_notify_origins:
                    try:
                        await self.context.send_message(origin, [Plain(msg_text)])
                        logger.info(f"å·²å‘ {origin} æ¨é€æ–°æ–°é—»")
                    except Exception as e:
                        logger.error(f"å‘é€æ¶ˆæ¯åˆ° {origin} å¤±è´¥ï¼š{str(e)}")
            else:
                logger.info("æœ¬æ¬¡æ£€æŸ¥æœªå‘ç°æ–°æ–°é—»æˆ–æ— è‡ªåŠ¨è®¢é˜…ä¼šè¯")
            await asyncio.sleep(interval)
    
    async def check_updates(self, force_update: bool = False):
        """
        æ›´æ–°æ–°é—»æ•°æ®ã€‚
        
        å‚æ•°:
          - force_update: è‹¥ä¸º Trueï¼Œåˆ™å…¨é‡æ›´æ–°ï¼ˆå¿½ç•¥æ•°æ®åº“åˆ¤æ–­ï¼‰ï¼›å¦åˆ™é‡åˆ°å·²æœ‰æ–°é—»æ—¶åœæ­¢å½“å‰æ ç›®çš„ç¿»é¡µã€‚
        
        è¿”å›:
          è¿”å›æœ¬æ¬¡æ›´æ–°ä¸­æ–°æ’å…¥çš„æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•æ ¼å¼ä¸º (æ¥æº, æ ç›®, æ ‡é¢˜, é“¾æ¥, å‘å¸ƒæ—¥æœŸ)ã€‚
        """
        new_news_all = []
        async with aiohttp.ClientSession() as session:
            for group in GROUPS:
                source = group["source"]
                base_url = group["base_url"]
                container_id = group["container_id"]
                logger.info(f"æ­£åœ¨çˆ¬å–ã€{source}ã€‘...")
                for cat_name, identifier in group["categories"].items():
                    logger.info(f"  æ ç›®ï¼š{cat_name} (æ ‡è¯†ï¼š{identifier})")
                    latest_date = None
                    if not force_update:
                        key = f"{source}:{cat_name}"
                        latest_date = self.db.get_latest_date(key)
                        logger.info(f"    æ•°æ®åº“ä¸­æœ€æ–°æ—¥æœŸä¸ºï¼š{latest_date}")
                    first_page_url = get_page_url(base_url, identifier, 1)
                    try:
                        async with session.get(first_page_url, headers=HEADERS) as resp:
                            if resp.status != 200:
                                logger.error(f"    è¯·æ±‚å¤±è´¥ï¼š{first_page_url} çŠ¶æ€ç ï¼š{resp.status}")
                                continue
                            first_text = await resp.text()
                    except Exception as e:
                        logger.error(f"    è¯·æ±‚ {first_page_url} å‡ºé”™ï¼š{str(e)}")
                        continue
                    soup = BeautifulSoup(first_text, "html.parser")
                    page_span = soup.find("span", class_="pages")
                    if page_span:
                        ems = page_span.find_all("em")
                        try:
                            total_pages = int(ems[-1].text.strip())
                        except Exception as e:
                            logger.error(f"    è§£ææ€»é¡µæ•°å¤±è´¥ï¼š{str(e)}")
                            total_pages = 1
                    else:
                        total_pages = 1
                    logger.info(f"    å…± {total_pages} é¡µ")
                    for page in range(1, total_pages + 1):
                        page_url = get_page_url(base_url, identifier, page)
                        logger.info(f"    æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µï¼š{page_url}")
                        try:
                            async with session.get(page_url, headers=HEADERS) as resp:
                                if resp.status != 200:
                                    logger.error(f"      ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status}")
                                    continue
                                page_text = await resp.text()
                        except Exception as e:
                            logger.error(f"      è¯·æ±‚ç¬¬ {page} é¡µå‡ºé”™ï¼š{str(e)}")
                            continue
                        soup = BeautifulSoup(page_text, "html.parser")
                        news_div = soup.find("div", id=container_id)
                        if not news_div:
                            logger.error(f"      æœªæ‰¾åˆ° id='{container_id}'ï¼Œè·³è¿‡ç¬¬ {page} é¡µ")
                            continue
                        page_news = []
                        # è‹¥å­˜åœ¨ ul.news_listï¼Œåˆ™é‡‡ç”¨åˆ—è¡¨ç»“æ„è§£æ
                        news_ul = news_div.find("ul", class_="news_list")
                        if news_ul:
                            for li in news_ul.find_all("li"):
                                title_span = li.find("span", class_="news_title")
                                if not title_span:
                                    title_span = li.find("span", class_="news_title5")
                                if not title_span:
                                    continue
                                a_tag = title_span.find("a")
                                if not a_tag:
                                    continue
                                title = a_tag.get("title", "").strip() or a_tag.text.strip()
                                href = a_tag.get("href", "").strip()
                                if not href:
                                    continue
                                date_span = li.find("span", class_="news_meta")
                                if not date_span:
                                    date_span = li.find("span", class_="news_meta1")
                                date_str = date_span.text.strip() if date_span else "æ—¥æœŸæœªçŸ¥"
                                full_url = href if href.startswith("http") else f"{base_url}{href}"
                                page_news.append((source, cat_name, title, full_url, date_str))
                        else:
                            # è¡¨æ ¼ç»“æ„è§£æ
                            for tr in news_div.find_all("tr"):
                                tds = tr.find_all("td", class_="main")
                                if len(tds) < 2:
                                    continue
                                title_tag = tds[0].find("a", title=True)
                                if not title_tag:
                                    links = tds[0].find_all("a")
                                    if len(links) >= 2:
                                        title_tag = links[1]
                                    else:
                                        continue
                                title = title_tag.get("title", "").strip() or title_tag.text.strip()
                                relative_url = title_tag.get("href", "").strip()
                                if not relative_url:
                                    continue
                                date_td = tds[-1]
                                div_date = date_td.find("div")
                                date_str = div_date.text.strip() if div_date else date_td.text.strip()
                                full_url = relative_url if relative_url.startswith("http") else f"{base_url}{relative_url}"
                                page_news.append((source, cat_name, title, full_url, date_str))
                        if page_news:
                            self.db.insert_news(page_news, key=f"{source}:{cat_name}")
                            new_news_all.extend(page_news)
                            if not force_update and latest_date:
                                try:
                                    page_dates = [datetime.strptime(n[4], "%Y-%m-%d") for n in page_news if n[4] != "æ—¥æœŸæœªçŸ¥"]
                                    if page_dates and min(page_dates) <= datetime.strptime(latest_date, "%Y-%m-%d"):
                                        logger.info(f"      {cat_name} ç¬¬ {page} é¡µè¾¾åˆ°å·²æœ‰æ–°é—»æ—¥æœŸï¼Œè·³å‡º")
                                        break
                                except Exception as e:
                                    logger.error(f"      æ—¥æœŸæ¯”è¾ƒå¤±è´¥ï¼š{str(e)}")
                        else:
                            break
                        await asyncio.sleep(1)
        logger.info(f"æœ¬æ¬¡æ›´æ–°å…±è·å– {len(new_news_all)} æ¡æ–°é—»")
        return new_news_all

    @filter.command("news")
    async def get_news(self, event: AstrMessageEvent, 
                       source: str = None, 
                       channel: str = None, 
                       page: int = 1, 
                       keyword: str = None, 
                       start_date: str = None, 
                       end_date: str = None):
        """
        è·å–æ–°é—»æŸ¥è¯¢ç»“æœã€‚
        
        å‚æ•°:
          - source: å¯é€‰ï¼ŒæŒ‡å®šæ–°é—»æ¥æºï¼ˆå¦‚ æ•™åŠ¡å¤„ã€å¤–å›½è¯­å­¦é™¢ã€ç”µå­ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢ï¼‰ã€‚
          - channel: å¯é€‰ï¼ŒæŒ‡å®šæ ç›®ï¼ˆå¦‚ zxdtã€å­¦é™¢å…¬å‘Šã€é€šçŸ¥å…¬å‘Šï¼‰ã€‚
          - page: é¡µç ï¼Œé»˜è®¤ 1ï¼Œæ¯é¡µæ˜¾ç¤º 5 æ¡æ–°é—»ã€‚
          - keyword: å¯é€‰ï¼Œæ ‡é¢˜å…³é”®è¯è¿‡æ»¤ã€‚
          - start_date: å¯é€‰ï¼Œèµ·å§‹å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDã€‚
          - end_date: å¯é€‰ï¼Œç»“æŸå‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDã€‚
        """
        per_page = 5
        news = self.db.get_news(source=source, channel=channel, page=page, per_page=per_page, 
                                keyword=keyword, start_date=start_date, end_date=end_date)
        if not news:
            yield event.plain_result("æš‚æ— æ›´å¤šæ–°é—»")
            return
        
        result = event.make_result().message(f"ğŸ“° æ–°é—»æŸ¥è¯¢ç»“æœï¼ˆç¬¬ {page} é¡µï¼‰\n")
        for idx, item in enumerate(news, 1):
            # å‡å®šæ–°é—»è®°å½•ç»“æ„ä¸º (æ¥æº, æ ç›®, æ ‡é¢˜, é“¾æ¥, å‘å¸ƒæ—¥æœŸ)
            result = result.message(f"{idx}. ã€{item[0]} - {item[1]}ã€‘{item[2]}\né“¾æ¥ï¼š{item[3]}\nå‘å¸ƒæ—¥æœŸï¼š{item[4]}\n\n")
        if len(news) == per_page:
            next_cmd = f"/news {source or ''} {channel or ''} {page+1} {keyword or ''} {start_date or ''} {end_date or ''}"
            result = result.message(f"å‘é€ {next_cmd.strip()} æŸ¥çœ‹ä¸‹ä¸€é¡µ")
        yield result

    @filter.command("news auto")
    async def news_auto_subscribe(self, event: AstrMessageEvent):
        """
        æŒ‡ä»¤: /news auto
        å°†å½“å‰ä¼šè¯åŠ å…¥è‡ªåŠ¨æ›´æ–°é€šçŸ¥åˆ—è¡¨ï¼ˆæŒä¹…åŒ–ï¼‰ã€‚
        """
        origin = event.unified_msg_origin
        if origin in self.auto_notify_origins:
            yield event.plain_result("å½“å‰ä¼šè¯å·²åœ¨è‡ªåŠ¨æ›´æ–°åˆ—è¡¨ä¸­ã€‚")
        else:
            self.auto_notify_origins.add(origin)
            save_auto_notify_origins(self.auto_notify_origins)
            yield event.plain_result("å·²å°†å½“å‰ä¼šè¯åŠ å…¥è‡ªåŠ¨æ›´æ–°é€šçŸ¥åˆ—è¡¨ã€‚")

    @filter.command("news auto off")
    async def news_auto_unsubscribe(self, event: AstrMessageEvent):
        """
        æŒ‡ä»¤: /news auto off
        å°†å½“å‰ä¼šè¯ä»è‡ªåŠ¨æ›´æ–°é€šçŸ¥åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆå¹¶æŒä¹…åŒ–ï¼‰ã€‚
        """
        origin = event.unified_msg_origin
        if origin in self.auto_notify_origins:
            self.auto_notify_origins.remove(origin)
            save_auto_notify_origins(self.auto_notify_origins)
            yield event.plain_result("å·²å°†å½“å‰ä¼šè¯ç§»é™¤è‡ªåŠ¨æ›´æ–°é€šçŸ¥åˆ—è¡¨ã€‚")
        else:
            yield event.plain_result("å½“å‰ä¼šè¯ä¸åœ¨è‡ªåŠ¨æ›´æ–°åˆ—è¡¨ä¸­ã€‚")

    @filter.command("news update")
    async def news_update(self, event: AstrMessageEvent):
        """
        æŒ‡ä»¤: /news update
        å¼ºåˆ¶å…¨é‡æ›´æ–°æ–°é—»ï¼Œæ— è®ºæ•°æ®åº“ä¸­æ˜¯å¦æœ€æ–°ï¼Œç„¶ååé¦ˆæ›´æ–°æ•°é‡ã€‚
        """
        new_news = await self.check_updates(force_update=True)
        msg = f"å…¨é‡æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(new_news)} æ¡æ–°é—»ã€‚"
        yield event.plain_result(msg)
    
    async def terminate(self):
        self.db.close()
